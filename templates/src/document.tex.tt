\documentclass[11pt]{article}
\usepackage{../include/mydefault}

\addbibresource{references.bib}

% Document metadata
\title{CMPUT 681 - Project: Game Solving with Parallel Counterfactual Regret Minimization}
\author{
  Dustin Morrill (Student number: 1200006, Unix ID: morrill)
}
\date{December 2, 2013}

\begin{document}
  \sloppy
  \maketitle
  \pagebreak

\section{Introduction}
Strategic interaction and decision making is at the heart of many exciting technologies. For instance, intelligent autonomous systems that can negotiate, bid in auctions, or manage the security forces of facilities such as airports~\cite{DBLP:journals/aim/PitaJOPTWPK09}.

Game theory is the mathematical study of strategic interactions between players who choose actions according to predefined rules to achieve payoffs. Game theoretic solution concepts, such as \emph{Nash equilibrium} strategies that are optimal against worst case opponents, provide guidance in finding desirable agent behaviour. Computational game theory, investigates effective and efficient methods of solving games to create intelligent systems. Inventing efficient solution methods and tools is of central importance to this field because solving complex, human-scale games is currently an intractable problem.

Much work has been done to find and improve solution algorithms. In particular, \emph{Counterfactual Regret Minimization} (\emph{CFR})~\cite{zinkevich2007new} has seen a great amount of interest~\cite{gibson2012efficient, johanson2012finding, Risk:2010:UCR:1838206.1838229} for its simplicity, memory efficiency, and its capacity to be easily tuned and combined with various sampling~\cite{gibson2012efficient, johanson2012efficient}, abstraction~\cite{Johanson:2013:ESA:2484920.2484965, DBLP:conf/aaai/HawkinHS12, Waugh2009}, and opponent modeling~\cite{bard2013online, johanson2007computing} techniques.

In order to solve more complicated games that encompass more ``real world'' qualities, CFR's performance must be improved. Introducing parallelism to CFR's \emph{game tree} traversal is one such method of doing so.

In this paper, I describe a new parallel version of CFR built to take advantage of natural parallelism in the CFR algorithm. Results from testing and evaluation on the game \emph{two-player limit Texas hold'em poker} are also presented. This game was chosen as a test domain because computer poker research is a pioneering field for computational game theory with annual international competitions and a natural benchmark in comparison with human performance.

\subsection{Background}
A game can be viewed as a graph of game state nodes and action or chance transition edges, rooted by the initial state and terminated by the final actions and payoffs. Walking this game tree from root to a leaf is equivalent to playing out an instance of the game. Payoffs for each player at leaves can be arbitrary in the general case, but for \emph{zero-sum} games, the gains of one player must be balanced by losses from another.

Some games have the notion of private or otherwise concealed information, such as face down private cards dealt to each player in Texas hold'em poker. For these \emph{imperfect information} games, game states that appear the same to a player due to hidden information are grouped into \emph{information sets}. As players can only make decisions based on the information available to them, only information sets, rather than their constituent game states, are strategically relevant.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{/home/dmorrill/workspace/ccnuma_cfr/report/diagrams/heads_up_limit.eps}
  \caption{Game tree of a small portion of a two-player limit Texas hold'em game from player 2's view. Player 2 does not observe which cards were dealt to player 1 (either the ace of hearts and king of hearts, or the two of spades and seven of diamonds, for example), its action must be made without this knowledge. The game states (circular nodes) joined by a dashed line are part of the same information set. The square leaf nodes show the payoffs for player 1 and 2 respectively.}
\end{figure}

\subsection{Algorithm Description}
CFR is an iterative \emph{self-play}\footnote{The game to be solved is played out repeatedly by agents who update their strategies in the same way after every match.} game solving algorithm that computes an approximate \emph{Nash equilibrium} strategy profile\footnote{Strategies for each player where neither player can gain by unilaterally changing strategy.} in two-player, zero-sum games~\cite{gibson2012efficient}. It does so by repeatedly traversing the game tree in a depth-first manner. As it walks down the tree from root to leaves, it updates the probabilities that each player will take actions to reach the current information set. When the game ends and a leaf payoff is encountered, it passes the value back up the tree to update its strategy at every traversed information set.

Rather than traversing the entire game tree on every iteration, the set of chance outcomes and player actions could instead be sampled. There are many different CFR sampling techniques including chance-sampling, average strategy sampling, and public chance sampling (PCS), which have been shown to converge faster in practice than no sampling ``vanilla'' CFR~\cite{Zinkevich2007a, gibson2012efficient, johanson2012efficient}.

The choice of sampling method determines the granularity of work processed on each iteration. The trade-off is that coarse grained and computationally intensive iterations, like those produced by PCS, have lower variance so CFR can converge in fewer iterations, while fine grained and computationally minimal iterations, like those produced by chance-sampling, have higher variance and require more iterations to converge.

The foundational techniques of this project are sampling method agnostic, but only chance-sampling was implemented because it is the simplest CFR variant\footnote{An attempt to implement PCS was made because it is an effective coarse grained sampling method~\cite{johanson2012efficient}. While it produced valid strategy profiles and showed promising speedups (for example, 6.37 with 8 cores in the 10 bucket abstraction (see section~\ref{sec:poker}) on 160 iterations), best response analysis showed that its strategy profiles do not converge to zero, so it is incorrectly implemented. As such, only results from the correctness validated chance-sampling are shown and discussed in this paper. However, the raw performance measurements of this incorrect PCS implementation are provided in appendix~\ref{ap:Pcs} for completeness.}.

Unfortunately, strategies in large games with many information sets and actions require billions of iterations to converge, which requires months on contemporary hardware, even with sampling. One of the ways in which CFR's performance could be improved is by introducing parallelism.

\section{Parallelizing CFR}
The key observation relevant to parallelizing CFR is that both the forward propagation of reach probabilities and the backwards propagation of game values in any particular subtree are independent to the traversal of every other subtree that is not a child or parent. This independence allows multiple threads to independently walk many subgames concurrently.

The only communication required is between parent and child subgames. The root of child subgames are leaves of their parents, so each child subgame must pass back the counterfactual value of its root in order for its parent to continue updating its regret values and strategy. Likewise, to begin their walks, parents must provide child subgames with the probabilities that players will act to reach their root, the chance actions sampled during this iteration, and the position (seat 1 or 2) of the strategy being updated on this iteration.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{/home/dmorrill/workspace/ccnuma_cfr/report/diagrams/general_subtree_partitioning.eps}
  \caption{Partition of a game tree into a parent trunk subgame and children subgames and data communication between them. Notice that each child subgame is independent from one another and only requires communication with its parent at the beginning and end of its subtree walk. Although not shown here and not implemented in this project, children subgames could themselves have child subgames.}
\end{figure}

\section{Implementation}
\subsection{Foundational Technologies}
This version of parallel CFR was implemented in \texttt{C++} using the \emph{Pthreads} library, building off of a card dealer, an information set indexing structure, and game evaluation routines provided by the \emph{Computer Poker Research Group} (\emph{CPRG}). In addition, the player action probability querying function, CFR's tree walk loop, and its strategy and regret update methods were modified from a CPRG CFR implementation.

\subsection{Algorithm Implementation}
The fundamental unit of CFR computation in this implementation is a \texttt{Subgame}\footnote{Because different sampling techniques are easier to implement and optimize using separate CFR tree walk methods, a generic \texttt{Subgame} is still an abstract notion. In practice, a \texttt{Subgame} may refer to a \texttt{ChanceSampledSubgame} or \texttt{PublicChanceSampledSubgame}. As such, a \texttt{SubgameInterface} superclass interface represents a generic \texttt{Subgame} in code.}. It is written in single program, multiple data (SPMD) form to encapsulate both sequential and parallel execution models.

In the sequential case, only one \texttt{Subgame} is created that spans the entire game tree. On every iteration, the \texttt{Subgame} walks from the root of the trunk to every terminal leaf that the opponent could play to reach, having been given the position of the strategy to update and card dealings\footnote{Here is where chance-sampling and PCS differ. Chance-sampling provides only a single sample of all cards to be dealt whereas PCS provides a single sample of public cards but all combinations of private cards.}. Because only lines of play possible with the current opponent action probabilities are traversed, branches of the tree are occasionally cut off.

In the parallel case, a \texttt{Subgame} is created in each thread. The main thread creates a trunk \texttt{Subgame} that spans from the root of the game to the roots of what I call remote \texttt{Subgame}s. These are the \texttt{Subgame}s created in the non-main threads and they are remote in the sense that they are only in indirect communication with the trunk \texttt{Subgame} through the \texttt{RemoteSugameRoot}. The remote \texttt{Subgame}s have a client-server relationship with the trunk \texttt{Subgame}.

\subsection{Communication}
The \texttt{RemoteSugameRoot} manages the forward propagation of reach probabilities, strategy positions, and card samples, as well as the backward propagation of game values between \texttt{Subgame}s. Synchronization between \texttt{Subgame}s is managed by queues within \texttt{RemoteSugameRoot}s with semaphores\footnote{In particular, this project uses the \emph{POSIX} semaphore library.}. A remote \texttt{Subgame} will wait on the forward propagation queue of its root in order to begin its portion of the tree walk. Upon completing its walk, the remote \texttt{Subgame} will have updated its regrets and strategy, and will push a new value onto its backward propagation queue so that its parent can continue its walk.

The set of \texttt{RemoteSugameRoot}s associated with remote \texttt{Subgame}s is encapsulated a \texttt{RemoteSubgameLeaves} instance. Upon reaching a new information set, a \texttt{Subgame} asks its \texttt{RemoteSubgameLeaves} instance if a non-terminal leaf, and therefore a remote \texttt{Subgame}'s root information set, has been encountered. If so, the \texttt{Subgame} pushes a forward update to the appropriate \texttt{RemoteSugameRoot} so that its child remote \texttt{Subgame} may begin its portion of the tree walk.

Pushes to both forward and backward queues include data copying overhead. While card samples can be shared amongst threads, both the strategy position and reach probabilities must be copied on a forward update. On a backward update, the counterfactual value (or values, depending on sampling technique) must be copied. For chance-sampling in a two-player game, this translates into copying three \texttt{doubles}, an \texttt{int}, and a pointer to the card samples for every forward-backward update pair, but for other sampling techniques, including PCS, the amount of copying required is greater. The cost of this copying is part of the communication overhead between the trunk and remote \texttt{Subgame}s, and as such must be amortized over comparatively long concurrent subtree traversals, as well as regret and strategy updates, for parallel CFR to be performant.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{/home/dmorrill/workspace/ccnuma_cfr/report/diagrams/subgame_communication.eps}
  \caption{Parallel CFR iteration design.}
\end{figure}

\subsection{Load Balancing}
Choice of leaf points between parent and child subgames determines load balancing and there are two dimensions to this choice: 1) how large the trunk should be and 2) how the crown\footnote{Area of a tree between the trunk and leaves.} should be partitioned amongst remote \texttt{Subgame}s.

\subsubsection{Trunk Size}
The trunk subgame completes its subtree walk in two phases. First, it updates reach probabilities at its leaves so that its children can begin their subtree walks. Once the remote \texttt{Subgame}s have returned their counterfactual values, the trunk updates its regrets and strategy\footnote{Note that this project makes no assumption about the order in which subtree paths are traversed. Marginal to moderate performance gains could be made by forward propagating reach probabilities to the remote \texttt{Subgame}s that will take the longest time to return before those that will be fast and backwards propagating from the fastest remote \texttt{Subgame}s first. Such optimization would increase CPU utilization and decrease the amount of time the trunk spends blocking on values from remote \texttt{Subgame}s, but is left as possible future work.}. A new iteration cannot begin until the trunk has completed its regret and strategy update because its path down the tree updating reach probabilities is determined by its regrets. Overlapping iterations would be algorithmically incorrect with respect to CFR.

Since remote \texttt{Subgame}s threads must block between iterations before they are provided a forward update from the trunk, better core utilization can be achieved by making the trunk very small. This makes both the forward propagation of reach probabilities and the backward update of regrets and strategy fast so that most of parallel CFR's time is spent concurrently walking subtrees rather than sequentially walking the trunk. However, the trunk must have at least as many branches as the number of desired remote \texttt{Subgame}s, so its minimum size is constrained by the desired amount of parallelism and the availability of physical resources.

As I used an \emph{SGI C2112-4G3} computing node composed of two 2.1 GHz dodeca-core \emph{AMD Opteron 6172} processors (24 cores total) and 32 GB of RAM, running the \emph{Red Hat 4.4.7-3} operating system, I was limited to 24 threads maximum. When more remote \texttt{Subgame} are desired than physical cores, subgames could be bundled together so that each thread manages more than one remote \texttt{Subgame}\footnote{Assigning more than one subgame to a thread would also help in balancing the computational load, since the average speeds of sets of subgames will be closer than the difference between the slowest and fastest subgames.}, but this is beyond the scope of this project and is left as future work. Instead, this project na\"{\i}vely assigns subgames to threads until the number of threads has run out and the rest of the tree is left as part of the trunk. While this produces a large, asymmetrical trunk, it still produces speedup when the granularity of remote \texttt{Subgame} work is large enough to keep them busy while the trunk is walking the full portions of the game tree.

As Neil Burch of the CPRG pointed out, the trunk can complete its two phases in separate threads to overlap portions of the reach probability update with the regret and strategy update. This technique was implemented, but was only used when more threads were available than remote \texttt{Subgame}s.

\subsubsection{Crown Partitioning}
Ideally, each core would work on a subgame of equal size with equal numbers of cutoffs\footnote{Unfortunately for load balancing, cutoffs are only known at runtime and change dynamically as strategies are updated, so perfect load balancing is not as simple as partitioning the game tree into equal size subgames.} so that each subtree walk takes roughly the same amount of time. Since each subgame must block between iterations waiting for forward updates from the trunk, and the trunk must receive updates from every remote \texttt{Subgame} before completing the iteration, the least idle time configuration occurs when all child subgames take the same time to traverse their subtrees.

With this ideal in mind, one can design a suitable domain specific crown partitioning. For poker, a convenient abstraction agnostic partitioning rule is to split subgames according to action sequences. In particular, the sequences that reach the beginning of a new round of betting. In Texas hold'em, each round except the initial round of the game (the ``preflop'') begins by revealing either three or one public cards: three on the second round (the ``flop''), one on the third round (the ``turn''), and one on the final round (the ``river''). Thus, splitting at the beginning of a new round allows remote \texttt{Subgame}s to concurrently traverse the different card combinations that were sampled on that round. For two-player limit poker, splitting on the action sequence at the beginning of a round has the added benefit of partitioning each subgame into equal sized subtrees. While the work done by each remote \texttt{Subgame} can be different because of cutoffs, this is the best partitioning that can be done without complicated dynamic subtree size analysis.

There are seven sequences that reach the beginning of the flop and 63 that reach the beginning of the turn. Limited to 24 cores, there would not be enough to run a remote \texttt{Subgame}s for each of the 63 turn root sequences. The performance of partitioning on turn root sequences with such a large asymmetrical trunk with 24 cores was poor\footnote{Even using (incorrect) PCS over 10 iterations, 24 cores obtained a speedup of 2.10 versus 5.55 for 8 cores.} so all following experiments were run partitioning on flop sequences. This explains that why, even though I ran on a 24 core machine, the maximum number of threads I tested was 9: one remote \texttt{Subgame} for each of the seven flop sequences, plus the main thread and a separate trunk reach probability updating thread.

\subsection{Poker as a Test Domain}\label{sec:poker}
Texas hold'em poker is a popular domain for testing game theoretic solving techniques. The \emph{Annual Computer Poker Competition} brings together researchers and hobbyists from around the world to evaluate their methods against each other in a tournament setting. In addition, comparison with human professionals is an intuitive way of measuring the progression of the field as a whole.

In terms of game properties, poker is interesting because it is a stochastic, imperfect information game with an emphasis on exploiting opponents. These properties make it significantly different than games traditionally studied in artificial intelligence research such as chess and checkers, and more representative of ``real world'' problems than them as well.

Even two-player limit Texas hold'em poker, a restrictive version with fixed wager sizes and a small fixed number of allowed wagers in each round, is too large to run CFR on directly. Instead, the game can be abstracted to be tractable for CFR. The typical abstraction in computer poker is to group similar card dealings together into ``buckets''~\cite{gibson2012efficient}. Zinkevich et al.~\cite{Zinkevich2007a}\ described an abstraction where card dealings with the same expected hand strength squared were placed in the same bucket. For the following experiments, 5 and 10 bucket versions of Zinkevich's abstractions where tested. The number of buckets denotes the branching factor of the game tree at each chance node~\cite{gibson2012efficient} and therefore also the relative size of each game. The 5 bucket abstraction is about 16\footnote{$3,624,290$ information sets for 5 buckets compared to $57,337,080$ information sets for 10 buckets. Calculated with a CPRG tool.} times smaller than the 10 bucket version.

\subsection{Correctness Validation}
The exploitability of strategies in the 5 bucket abstraction created by running parallel CFR with varying numbers of iterations with 1 to 9 threads were measured using a CPRG tool. If parallel CFR is implemented properly, the exploitability must converge to zero as the number of iterations approaches infinity. Although exploitability as a function of iterations may not be monotonically decreasing, a downward trend should be observed over many iterations. As shown by figure \ref{fig:convergence}, both sequential and parallel versions do appear to properly generate strategy profiles that better approximate a \emph{Nash equilibrium} as the number of iterations increase.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{/home/dmorrill/workspace/ccnuma_cfr/experiments/experiment.Nov30_2013.15-40-55/results/convergence.eps}
  \caption{Exploitability of chance-sampled parallel CFR generated strategies in the abstract 5 bucket game in milliblinds per game (mb/g), where blinds are 10 chips each. A blind is the cost that must be paid at the beginning of a hand before any cards are dealt and it determines initial wager size. }
  \label{fig:convergence}
\end{figure}%

\section{Empirical Analysis}

\subsection{Experimental Design}
The performance of parallel CFR with both the 5 and 10 bucket abstractions of two-player limit Texas hold'em was measured when 1 to 9 threads were used. The time to complete various numbers of iterations (1 thousand, 2 thousand, 4 thousand, 8 thousand, 16 thousand, and 32 thousand) were recored and analyzed. The results from every different number of iterations were similar so only the results from the largest number iterations (32 thousand) are shown here. See appendix \ref{ap:Cs} for the results of omitted configurations.

To reduce inconsistency in results, two cold runs of each experimental configuration were executed before averaging five more runs, each with a different set of random seeds for the CPRG card sampling engine. Each experiment was run on an \emph{SGI C2112-4G3} computing node composed of two 2.1 GHz dodeca-core \emph{AMD Opteron 6172} processors (24 cores total) and 32 GB of RAM, running the \emph{Red Hat 4.4.7-3} operating system. Parallel CFR and its CPRG components were compiled with \texttt{gcc} version 4.4.7.

\subsection{Results}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{/home/dmorrill/workspace/ccnuma_cfr/experiments/experiment.Nov30_2013.15-40-55/results/speedup.iter-32k.eps}
  \caption{Speedup for each configuration during 32 thousand iterations (the largest number of iterations run), plus linear and no speedup for comparison.}
  \label{fig:convergence}
\end{figure}%
\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{/home/dmorrill/workspace/ccnuma_cfr/experiments/experiment.Nov30_2013.15-40-55/results/realTime.iter-32k.eps}
  \caption{Real times in microseconds for each configuration to complete 32 thousand iterations.}
  \label{fig:sub2}
\end{figure}

The speedup of parallel CFR with chance-sampling is disappointing, but is somewhat expected. Chance-sampling is a fine grained sampling technique, particularly in two-player limit poker where the action space is highly constrained\footnote{Two-player limit poker has a maximum of three actions per decision and four wagers per round.}. The amount of work provided to each remote \texttt{Subgame} on each iteration is too small for parallelism to gain much against the overhead of semaphore synchronized communication and data copying, as well as the idle time incurred by imperfect load balancing. Using a coarse grained sampling method like PCS would almost certainly improve performance\footnote{This claim is empirically supported by the results of the partially complete PCS implementation in appendix~\ref{ap:Pcs}.}.

As expected, the best performance was observed with 8 cores, since this allowed parallel CFR to use as many remote \texttt{Subgame}s as the flop sequence branching rule dictates. Using less than 8 cores, the trunk is asymmetrical, so remote \texttt{Subgame}s sit idle longer waiting for the trunk to finish traversing the long portions of its tree.

Although completing the two phases of the trunk update in separate threads could be a very useful technique to deal with a large trunk and many remote \texttt{Subgame}s, it was not effective with the small trunk used in this project. In fact, the 9 core point where this was done shows that it was counterproductive. The overhead of communicating with the reach probability updating thread was higher than the gain from concurrency. If this were not the case, 9 cores would be no slower than 8 cores. The reach probability updating phase of the trunk iteration is so much faster than the child subgame walks that a single thread completes it, begins the walk to update the trunk regrets and strategy, and waits for the first remote \texttt{Subgame} to push a backward update in the same time as two trunk threads doing the forward and backward updates in parallel. Doing both trunk updates in a single thread avoids the communication overheads associated with pushing a forward update, making the 8 threaded configuration more performant than the 9 threaded.

The size of the game, the 5s abstraction versus the 10s, had little impact on both real time and speedup because when sampling chance outcomes there is little difference between them in terms of time per iteration. The regret and strategy updates will take longer in the 10s abstraction since there are more information sets, but the game tree walk to update reach probabilities and counterfactual values is the same size in both cases.

\section{Conclusion}
CFR is a recently developed but well studied iterative algorithm for solving games that required too much memory or time to solve with earlier techniques. Parallelizing CFR by making use of the parallel structure of its game tree walk can make solving even larger, more complicated games practical.

Many sampling variants of CFR exist and choice of variant will have a heavy influence on parallel CFR's performance. Since the sampling method determines how thorough an update is done on each iteration, it is the primary influence on granularity. Sampling methods that produce coarse grained, computationally intensive, but lower variance iterations, such as PCS, would allow remote \texttt{Subgame}s to work concurrently for a greater proportion of CFR's total runtime, than those like chance-sampling that produce fine grained, computationally minimal, but higher variance iterations.

While the performance of chance-sampled parallel CFR was lacklustre, showing any speedup at all with a fine grained update method such as chance-sampling is a noteworthy result. Empirical results aside, parallel CFR stands as both a proof of concept and a strong foundation for future work.


\section{Acknowledgments}
I would like to thank the CPRG for use of a card dealer class, an information set indexing structure, game evaluation routines, an information set counting tool, an exploitability calculating tool, and a previous implementation of CFR that was used as a template for parallel CFR. Furthermore, I would like to thank the CPRG members, in particular Michael Johanson and Neil Burch for their help in designing parallel CFR, in using CPRG tools, and in interfacing with CPRG code.

Computational resources for empirical analysis were provided by Calcul Quebec and Compute Canada.

\printbibliography

\appendix

\section{Chance-sampled Performance Measurements}\label{ap:Cs}

\begin{verbatim}
# iterations   abstraction cores   avgRealTime(s)  speedup         efficiency
1 Thousand     5s          1       1.18            1               1
1 Thousand     5s          2       1.51            0.777           0.388
1 Thousand     5s          3       1.31            0.9             0.3
1 Thousand     5s          4       1.15            1.03            0.256
1 Thousand     5s          5       0.951           1.24            0.248
1 Thousand     5s          6       0.788           1.49            0.249
1 Thousand     5s          7       0.625           1.88            0.269
1 Thousand     5s          8       0.487           2.42            0.302
1 Thousand     5s          9       0.631           1.87            0.207
1 Thousand     10s         1       1.68            1               1
1 Thousand     10s         2       2.07            0.81            0.405
1 Thousand     10s         3       1.86            0.905           0.302
1 Thousand     10s         4       1.59            1.06            0.264
1 Thousand     10s         5       1.29            1.3             0.261
1 Thousand     10s         6       1.01            1.66            0.277
1 Thousand     10s         7       0.82            2.05            0.293
1 Thousand     10s         8       0.616           2.73            0.341
1 Thousand     10s         9       0.797           2.11            0.234
2 Thousand     5s          1       2               1               1
2 Thousand     5s          2       2.63            0.76            0.38
2 Thousand     5s          3       2.32            0.863           0.288
2 Thousand     5s          4       2.01            0.994           0.248
2 Thousand     5s          5       1.65            1.21            0.242
2 Thousand     5s          6       1.37            1.45            0.242
2 Thousand     5s          7       1.13            1.77            0.253
2 Thousand     5s          8       0.865           2.31            0.289
2 Thousand     5s          9       1.09            1.83            0.204
2 Thousand     10s         1       2.88            1               1
2 Thousand     10s         2       3.64            0.792           0.396
2 Thousand     10s         3       3.23            0.894           0.298
2 Thousand     10s         4       2.73            1.06            0.264
2 Thousand     10s         5       2.26            1.27            0.255
2 Thousand     10s         6       1.82            1.59            0.265
2 Thousand     10s         7       1.47            1.96            0.28
2 Thousand     10s         8       1.13            2.55            0.319
2 Thousand     10s         9       1.44            2               0.223
4 Thousand     5s          1       3.56            1               1
4 Thousand     5s          2       4.7             0.756           0.378
4 Thousand     5s          3       4.14            0.859           0.286
4 Thousand     5s          4       3.58            0.994           0.248
4 Thousand     5s          5       2.94            1.21            0.242
4 Thousand     5s          6       2.47            1.44            0.24
4 Thousand     5s          7       2.07            1.72            0.246
4 Thousand     5s          8       1.59            2.23            0.279
4 Thousand     5s          9       1.98            1.79            0.199
4 Thousand     10s         1       5.16            1               1
4 Thousand     10s         2       6.47            0.798           0.399
4 Thousand     10s         3       5.67            0.91            0.303
4 Thousand     10s         4       4.88            1.06            0.264
4 Thousand     10s         5       3.99            1.29            0.259
4 Thousand     10s         6       3.34            1.55            0.258
4 Thousand     10s         7       2.63            1.96            0.281
4 Thousand     10s         8       2.07            2.49            0.311
4 Thousand     10s         9       2.66            1.94            0.216
8 Thousand     5s          1       6.58            1               1
8 Thousand     5s          2       8.67            0.759           0.38
8 Thousand     5s          3       7.71            0.854           0.285
8 Thousand     5s          4       6.69            0.984           0.246
8 Thousand     5s          5       5.49            1.2             0.24
8 Thousand     5s          6       4.67            1.41            0.235
8 Thousand     5s          7       3.81            1.73            0.247
8 Thousand     5s          8       2.94            2.24            0.28
8 Thousand     5s          9       3.61            1.82            0.202
8 Thousand     10s         1       8.83            1               1
8 Thousand     10s         2       11.5            0.771           0.386
8 Thousand     10s         3       9.85            0.897           0.299
8 Thousand     10s         4       8.73            1.01            0.253
8 Thousand     10s         5       7.05            1.25            0.25
8 Thousand     10s         6       6.02            1.47            0.245
8 Thousand     10s         7       4.66            1.9             0.271
8 Thousand     10s         8       3.77            2.35            0.293
8 Thousand     10s         9       4.87            1.81            0.202
16 Thousand    5s          1       12.2            1               1
16 Thousand    5s          2       16.6            0.737           0.368
16 Thousand    5s          3       14.9            0.818           0.273
16 Thousand    5s          4       12.8            0.954           0.238
16 Thousand    5s          5       10.4            1.17            0.234
16 Thousand    5s          6       9.11            1.34            0.224
16 Thousand    5s          7       7.45            1.64            0.234
16 Thousand    5s          8       5.61            2.18            0.272
16 Thousand    5s          9       6.89            1.77            0.197
16 Thousand    10s         1       15.1            1               1
16 Thousand    10s         2       20              0.752           0.376
16 Thousand    10s         3       17              0.884           0.295
16 Thousand    10s         4       15.3            0.982           0.246
16 Thousand    10s         5       12.4            1.22            0.243
16 Thousand    10s         6       10.6            1.42            0.237
16 Thousand    10s         7       8.42            1.79            0.256
16 Thousand    10s         8       6.61            2.28            0.285
16 Thousand    10s         9       8.62            1.75            0.194
32 Thousand    5s          1       24.2            1               1
32 Thousand    5s          2       33.8            0.714           0.357
32 Thousand    5s          3       29.6            0.816           0.272
32 Thousand    5s          4       24.8            0.972           0.243
32 Thousand    5s          5       20.9            1.15            0.231
32 Thousand    5s          6       18.5            1.3             0.217
32 Thousand    5s          7       14.8            1.64            0.234
32 Thousand    5s          8       11.4            2.13            0.266
32 Thousand    5s          9       13.1            1.85            0.205
32 Thousand    10s         1       27.3            1               1
32 Thousand    10s         2       36.5            0.747           0.373
32 Thousand    10s         3       31.2            0.873           0.291
32 Thousand    10s         4       27.9            0.977           0.244
32 Thousand    10s         5       22.9            1.19            0.238
32 Thousand    10s         6       18.9            1.45            0.241
32 Thousand    10s         7       15.9            1.72            0.246
32 Thousand    10s         8       12.1            2.25            0.281
32 Thousand    10s         9       16.1            1.69            0.188
\end{verbatim}

\section{Public Chance Sampled Performance Measurements}\label{ap:Pcs}
\begin{verbatim}
# iterations   abstraction cores   avgRealTime(s)  speedup         efficiency
10             5s          1       9.89            1               1
10             5s          2       14.4            0.687           0.343
10             5s          3       9.24            1.07            0.356
10             5s          4       8.4             1.18            0.294
10             5s          5       6.61            1.5             0.299
10             5s          6       4.82            2.05            0.342
10             5s          7       3.3             2.99            0.428
10             5s          8       1.78            5.55            0.694
10             5s          9       1.81            5.47            0.608
10             10s         1       12.7            1               1
10             10s         2       18.1            0.699           0.35
10             10s         3       15.1            0.84            0.28
10             10s         4       12.1            1.04            0.261
10             10s         5       9.44            1.34            0.268
10             10s         6       6.4             1.98            0.329
10             10s         7       4.21            3.01            0.43
10             10s         8       2.26            5.6             0.7
10             10s         9       2.27            5.58            0.62
20             5s          1       22.7            1               1
20             5s          2       32.7            0.694           0.347
20             5s          3       19.1            1.19            0.396
20             5s          4       18.2            1.25            0.312
20             5s          5       15.3            1.49            0.297
20             5s          6       10.4            2.17            0.362
20             5s          7       7.34            3.09            0.442
20             5s          8       3.82            5.94            0.742
20             5s          9       3.92            5.78            0.643
20             10s         1       28.2            1               1
20             10s         2       38.5            0.732           0.366
20             10s         3       33.5            0.842           0.281
20             10s         4       26.2            1.08            0.269
20             10s         5       20.6            1.37            0.273
20             10s         6       13.9            2.03            0.338
20             10s         7       9               3.13            0.448
20             10s         8       4.77            5.91            0.739
20             10s         9       4.79            5.89            0.654
40             5s          1       48.3            1               1
40             5s          2       69              0.699           0.35
40             5s          3       42.1            1.15            0.382
40             5s          4       38.9            1.24            0.31
40             5s          5       33.2            1.45            0.291
40             5s          6       22.6            2.14            0.356
40             5s          7       15.6            3.09            0.441
40             5s          8       8.08            5.97            0.747
40             5s          9       8.36            5.77            0.641
40             10s         1       64.8            1               1
40             10s         2       85.6            0.757           0.378
40             10s         3       73.3            0.884           0.295
40             10s         4       58.2            1.11            0.278
40             10s         5       43.2            1.5             0.3
40             10s         6       30.5            2.12            0.353
40             10s         7       19.5            3.32            0.474
40             10s         8       10.3            6.27            0.784
40             10s         9       10.4            6.25            0.694
80             5s          1       99.2            1               1
80             5s          2       142             0.701           0.351
80             5s          3       94              1.06            0.352
80             5s          4       79.4            1.25            0.312
80             5s          5       67.7            1.47            0.293
80             5s          6       46.4            2.14            0.357
80             5s          7       31.7            3.13            0.447
80             5s          8       16.3            6.09            0.761
80             5s          9       16.8            5.9             0.656
80             10s         1       137             1               1
80             10s         2       185             0.736           0.368
80             10s         3       156             0.874           0.291
80             10s         4       125             1.09            0.272
80             10s         5       95.5            1.43            0.286
80             10s         6       64.6            2.11            0.352
80             10s         7       41.1            3.32            0.474
80             10s         8       21.8            6.26            0.782
80             10s         9       21.9            6.23            0.692
160            5s          1       198             1               1
160            5s          2       280             0.708           0.354
160            5s          3       200             0.993           0.331
160            5s          4       157             1.27            0.317
160            5s          5       133             1.49            0.299
160            5s          6       92.1            2.15            0.359
160            5s          7       63              3.15            0.45
160            5s          8       31.9            6.22            0.777
160            5s          9       32.8            6.05            0.672
160            10s         1       287             1               1
160            10s         2       385             0.745           0.373
160            10s         3       325             0.885           0.295
160            10s         4       260             1.11            0.277
160            10s         5       198             1.45            0.291
160            10s         6       132             2.18            0.363
160            10s         7       84.7            3.39            0.485
160            10s         8       45.1            6.37            0.796
160            10s         9       44.8            6.41            0.712
\end{verbatim}

\section{Chance-sampled Exploitability Data}
\begin{verbatim}
# cores   iterations     exploitability(mb/g)
1         1 Thousand     717
1         2 Thousand     822
1         4 Thousand     811
1         8 Thousand     668
1         16 Thousand    431
1         32 Thousand    296
2         1 Thousand     470
2         2 Thousand     558
2         4 Thousand     507
2         8 Thousand     451
2         16 Thousand    328
2         32 Thousand    298
3         1 Thousand     790
3         2 Thousand     854
3         4 Thousand     650
3         8 Thousand     430
3         16 Thousand    243
3         32 Thousand    184
4         1 Thousand     347
4         2 Thousand     310
4         4 Thousand     323
4         8 Thousand     291
4         16 Thousand    220
4         32 Thousand    205
5         1 Thousand     344
5         2 Thousand     380
5         4 Thousand     379
5         8 Thousand     335
5         16 Thousand    223
5         32 Thousand    199
6         1 Thousand     193
6         2 Thousand     174
6         4 Thousand     163
6         8 Thousand     195
6         16 Thousand    176
6         32 Thousand    165
7         1 Thousand     223
7         2 Thousand     242
7         4 Thousand     212
7         8 Thousand     200
7         16 Thousand    174
7         32 Thousand    164
8         1 Thousand     591
8         2 Thousand     720
8         4 Thousand     747
8         8 Thousand     639
8         16 Thousand    419
8         32 Thousand    289
9         1 Thousand     720
9         2 Thousand     555
9         4 Thousand     651
9         8 Thousand     545
9         16 Thousand    617
9         32 Thousand    383
\end{verbatim}

\section{Public Chance Sampled Exploitability Data}
\begin{verbatim}
# cores   iterations     exploitability(mb/g)
1         10             788
1         20             802
1         40             795
1         80             810
1         160            842
2         10             779
2         20             730
2         40             699
2         80             702
2         160            699
3         10             735
3         20             696
3         40             679
3         80             684
3         160            662
4         10             610
4         20             675
4         40             705
4         80             718
4         160            720
5         10             727
5         20             645
5         40             618
5         80             622
5         160            633
6         10             742
6         20             711
6         40             703
6         80             715
6         160            742
7         10             744
7         20             707
7         40             669
7         80             674
7         160            694
8         10             768
8         20             812
8         40             801
8         80             966
8         160            952
9         10             782
9         20             870
9         40             840
9         80             937
9         160            990
\end{verbatim}

\end{document}
